name: Test GitHub Action

on:
  push:
    branches: [ main, develop ]
    paths: 
      - 'action.yml'
      - 'action.sh'
      - '.github/workflows/test-action.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'action.yml'
      - 'action.sh'
      - '.github/workflows/test-action.yml'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

jobs:
  # Unit tests for action components
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Run Unit Tests
      run: ./test/action_test.sh

  # Test action with different scenarios
  integration-tests:
    name: Integration Tests
    runs-on: ${{ matrix.os }}
    needs: unit-tests
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        terraform-version: ['1.6.0', '1.7.0', 'latest']
        scenario: ['basic', 'with-changes', 'with-dangers']
      fail-fast: false
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ matrix.terraform-version }}
        
    - name: Create Test Terraform Configuration
      shell: bash
      run: |
        mkdir -p test-terraform
        cd test-terraform
        
        case "${{ matrix.scenario }}" in
          "basic")
            cat > main.tf << 'EOF'
        # Basic configuration with no changes
        terraform {
          required_providers {
            local = {
              source = "hashicorp/local"
              version = "~> 2.0"
            }
          }
        }
        
        resource "local_file" "test" {
          content  = "Hello, World!"
          filename = "${path.module}/hello.txt"
        }
        EOF
            ;;
            
          "with-changes")
            cat > main.tf << 'EOF'
        # Configuration with multiple changes
        terraform {
          required_providers {
            local = {
              source = "hashicorp/local"
              version = "~> 2.0"
            }
          }
        }
        
        resource "local_file" "test1" {
          content  = "Hello, World 1!"
          filename = "${path.module}/hello1.txt"
        }
        
        resource "local_file" "test2" {
          content  = "Hello, World 2!"
          filename = "${path.module}/hello2.txt"
        }
        
        resource "local_file" "test3" {
          content  = "Hello, World 3!"
          filename = "${path.module}/hello3.txt"
        }
        EOF
            ;;
            
          "with-dangers")
            cat > main.tf << 'EOF'
        # Configuration that might trigger danger warnings
        terraform {
          required_providers {
            local = {
              source = "hashicorp/local"
              version = "~> 2.0"
            }
          }
        }
        
        # Multiple resources that will be created
        resource "local_file" "danger1" {
          content  = "Danger 1"
          filename = "${path.module}/danger1.txt"
        }
        
        resource "local_file" "danger2" {
          content  = "Danger 2"
          filename = "${path.module}/danger2.txt"
        }
        
        resource "local_file" "danger3" {
          content  = "Danger 3"
          filename = "${path.module}/danger3.txt"
        }
        
        resource "local_file" "danger4" {
          content  = "Danger 4"
          filename = "${path.module}/danger4.txt"
        }
        
        resource "local_file" "danger5" {
          content  = "Danger 5"
          filename = "${path.module}/danger5.txt"
        }
        EOF
            ;;
        esac
        
    - name: Create Strata Configuration
      shell: bash
      run: |
        cat > test-terraform/.strata.yaml << 'EOF'
        output: markdown
        plan:
          danger-threshold: 3
          show-details: true
          highlight-dangers: true
        
        sensitive_resources:
          - resource_type: local_file
        EOF
        
    - name: Terraform Init
      shell: bash
      run: |
        cd test-terraform
        terraform init
        
    - name: Terraform Plan
      shell: bash
      run: |
        cd test-terraform
        terraform plan -out=terraform.tfplan
        
    - name: Test Action - Basic Usage
      id: test-basic
      uses: ./
      with:
        plan-file: test-terraform/terraform.tfplan
        
    - name: Test Action - With Details
      id: test-details
      uses: ./
      with:
        plan-file: test-terraform/terraform.tfplan
        show-details: true
        
    - name: Test Action - With Custom Config
      id: test-config
      uses: ./
      with:
        plan-file: test-terraform/terraform.tfplan
        config-file: test-terraform/.strata.yaml
        
    - name: Test Action - JSON Output
      id: test-json
      uses: ./
      with:
        plan-file: test-terraform/terraform.tfplan
        output-format: json
        
    - name: Test Action - Custom Danger Threshold
      id: test-threshold
      uses: ./
      with:
        plan-file: test-terraform/terraform.tfplan
        danger-threshold: 2
        
    - name: Test Action - No PR Comments
      id: test-no-comments
      uses: ./
      with:
        plan-file: test-terraform/terraform.tfplan
        comment-on-pr: false
        
    - name: Validate Outputs
      shell: bash
      run: |
        echo "Testing action outputs..."
        
        # Test that outputs are set
        if [ -z "${{ steps.test-basic.outputs.summary }}" ]; then
          echo "ERROR: summary output is empty"
          exit 1
        fi
        
        if [ -z "${{ steps.test-basic.outputs.has-changes }}" ]; then
          echo "ERROR: has-changes output is empty"
          exit 1
        fi
        
        if [ -z "${{ steps.test-basic.outputs.has-dangers }}" ]; then
          echo "ERROR: has-dangers output is empty"
          exit 1
        fi
        
        if [ -z "${{ steps.test-basic.outputs.change-count }}" ]; then
          echo "ERROR: change-count output is empty"
          exit 1
        fi
        
        if [ -z "${{ steps.test-basic.outputs.danger-count }}" ]; then
          echo "ERROR: danger-count output is empty"
          exit 1
        fi
        
        # Test JSON output format
        if [ -z "${{ steps.test-json.outputs.json-summary }}" ]; then
          echo "ERROR: json-summary output is empty"
          exit 1
        fi
        
        # Validate JSON format
        echo '${{ steps.test-json.outputs.json-summary }}' | jq . > /dev/null
        if [ $? -ne 0 ]; then
          echo "ERROR: json-summary is not valid JSON"
          exit 1
        fi
        
        echo "All output validation tests passed!"
        
    - name: Test Scenario-Specific Expectations
      shell: bash
      run: |
        case "${{ matrix.scenario }}" in
          "basic")
            # Basic scenario should have changes but no dangers
            if [ "${{ steps.test-basic.outputs.has-changes }}" != "true" ]; then
              echo "ERROR: Basic scenario should have changes"
              exit 1
            fi
            ;;
            
          "with-changes")
            # Should have multiple changes
            change_count="${{ steps.test-basic.outputs.change-count }}"
            if [ "$change_count" -lt "3" ]; then
              echo "ERROR: Expected at least 3 changes, got $change_count"
              exit 1
            fi
            ;;
            
          "with-dangers")
            # Should trigger danger threshold with 5 resources
            change_count="${{ steps.test-threshold.outputs.change-count }}"
            if [ "$change_count" -lt "5" ]; then
              echo "ERROR: Expected at least 5 changes, got $change_count"
              exit 1
            fi
            ;;
        esac
        
        echo "Scenario-specific tests passed for ${{ matrix.scenario }}!"

  # Test error handling scenarios
  error-handling-tests:
    name: Error Handling Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Test Missing Plan File
      id: test-missing-file
      uses: ./
      continue-on-error: true
      with:
        plan-file: nonexistent.tfplan
        
    - name: Validate Error Handling
      shell: bash
      run: |
        # The action should fail gracefully for missing files
        if [ "${{ steps.test-missing-file.outcome }}" != "failure" ]; then
          echo "ERROR: Action should fail for missing plan file"
          exit 1
        fi
        
        echo "Error handling test passed!"

  # Test different input combinations
  input-validation-tests:
    name: Input Validation Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      
    - name: Create Simple Terraform Configuration
      run: |
        cat > main.tf << 'EOF'
        terraform {
          required_providers {
            local = {
              source = "hashicorp/local"
              version = "~> 2.0"
            }
          }
        }
        
        resource "local_file" "test" {
          content  = "Test content"
          filename = "${path.module}/test.txt"
        }
        EOF
        
    - name: Terraform Init and Plan
      run: |
        terraform init
        terraform plan -out=terraform.tfplan
        
    - name: Test Invalid Output Format
      id: test-invalid-format
      uses: ./
      with:
        plan-file: terraform.tfplan
        output-format: invalid-format
        
    - name: Test Invalid Boolean Values
      id: test-invalid-boolean
      uses: ./
      with:
        plan-file: terraform.tfplan
        show-details: invalid-boolean
        comment-on-pr: not-a-boolean
        
    - name: Test Invalid Danger Threshold
      id: test-invalid-threshold
      uses: ./
      with:
        plan-file: terraform.tfplan
        danger-threshold: not-a-number
        
    - name: Test Nonexistent Config File
      id: test-invalid-config
      uses: ./
      with:
        plan-file: terraform.tfplan
        config-file: nonexistent-config.yaml
        
    - name: Validate Input Handling
      shell: bash
      run: |
        # All tests should succeed despite invalid inputs (graceful handling)
        if [ "${{ steps.test-invalid-format.outcome }}" != "success" ]; then
          echo "ERROR: Action should handle invalid output format gracefully"
          exit 1
        fi
        
        if [ "${{ steps.test-invalid-boolean.outcome }}" != "success" ]; then
          echo "ERROR: Action should handle invalid boolean values gracefully"
          exit 1
        fi
        
        if [ "${{ steps.test-invalid-threshold.outcome }}" != "success" ]; then
          echo "ERROR: Action should handle invalid danger threshold gracefully"
          exit 1
        fi
        
        if [ "${{ steps.test-invalid-config.outcome }}" != "success" ]; then
          echo "ERROR: Action should handle nonexistent config file gracefully"
          exit 1
        fi
        
        echo "Input validation tests passed!"

  # Test PR comment functionality (only on PRs)
  pr-comment-tests:
    name: PR Comment Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      
    - name: Create Terraform Configuration
      run: |
        cat > main.tf << 'EOF'
        terraform {
          required_providers {
            local = {
              source = "hashicorp/local"
              version = "~> 2.0"
            }
          }
        }
        
        resource "local_file" "pr_test" {
          content  = "PR test content"
          filename = "${path.module}/pr_test.txt"
        }
        EOF
        
    - name: Terraform Init and Plan
      run: |
        terraform init
        terraform plan -out=terraform.tfplan
        
    - name: Test PR Comment Creation
      id: test-pr-comment
      uses: ./
      with:
        plan-file: terraform.tfplan
        comment-on-pr: true
        comment-header: "ðŸ§ª Test PR Comment"
        
    - name: Test PR Comment Update
      id: test-pr-update
      uses: ./
      with:
        plan-file: terraform.tfplan
        comment-on-pr: true
        update-comment: true
        comment-header: "ðŸ§ª Updated Test PR Comment"
        
    - name: Validate PR Comment Tests
      shell: bash
      run: |
        if [ "${{ steps.test-pr-comment.outcome }}" != "success" ]; then
          echo "ERROR: PR comment creation failed"
          exit 1
        fi
        
        if [ "${{ steps.test-pr-update.outcome }}" != "success" ]; then
          echo "ERROR: PR comment update failed"
          exit 1
        fi
        
        echo "PR comment tests passed!"

  # Performance and caching tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      
    - name: Create Terraform Configuration
      run: |
        cat > main.tf << 'EOF'
        terraform {
          required_providers {
            local = {
              source = "hashicorp/local"
              version = "~> 2.0"
            }
          }
        }
        
        resource "local_file" "perf_test" {
          content  = "Performance test"
          filename = "${path.module}/perf_test.txt"
        }
        EOF
        
    - name: Terraform Init and Plan
      run: |
        terraform init
        terraform plan -out=terraform.tfplan
        
    - name: First Run (Cache Miss)
      id: first-run
      uses: ./
      with:
        plan-file: terraform.tfplan
        
    - name: Second Run (Cache Hit)
      id: second-run
      uses: ./
      with:
        plan-file: terraform.tfplan
        
    - name: Validate Performance Tests
      shell: bash
      run: |
        if [ "${{ steps.first-run.outcome }}" != "success" ]; then
          echo "ERROR: First run failed"
          exit 1
        fi
        
        if [ "${{ steps.second-run.outcome }}" != "success" ]; then
          echo "ERROR: Second run failed"
          exit 1
        fi
        
        echo "Performance tests passed!"

  # Summary job
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, error-handling-tests, input-validation-tests, performance-tests]
    if: always()
    
    steps:
    - name: Test Results Summary
      shell: bash
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.unit-tests.result }}" == "success" ]; then
          echo "âœ… Unit Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Unit Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.integration-tests.result }}" == "success" ]; then
          echo "âœ… Integration Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Integration Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.error-handling-tests.result }}" == "success" ]; then
          echo "âœ… Error Handling Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Error Handling Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.input-validation-tests.result }}" == "success" ]; then
          echo "âœ… Input Validation Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Input Validation Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.performance-tests.result }}" == "success" ]; then
          echo "âœ… Performance Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Performance Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Overall Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARYname: Test GitHub Action

on:
  push:
    branches: [ main, develop ]
    paths: 
      - 'action.yml'
      - 'action.sh'
      - '.github/workflows/test-action.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'action.yml'
      - 'action.sh'
      - '.github/workflows/test-action.yml'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

jobs:
  # Test the action with different scenarios
  test-action:
    name: Test Action - ${{ matrix.scenario }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
        scenario:
          - basic
          - detailed
          - custom-config
          - json-output
        include:
          - scenario: basic
            plan-file: samples/websample.json
            output-format: markdown
            show-details: false
            comment-on-pr: false
          - scenario: detailed
            plan-file: samples/k8ssample.json
            output-format: markdown
            show-details: true
            comment-on-pr: false
          - scenario: custom-config
            plan-file: samples/websample.json
            output-format: table
            show-details: true
            comment-on-pr: false
            config-file: strata.yaml
          - scenario: json-output
            plan-file: samples/k8ssample.json
            output-format: json
            show-details: false
            comment-on-pr: false

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Verify test plan file exists
      run: |
        if [ ! -f "${{ matrix.plan-file }}" ]; then
          echo "Test plan file not found: ${{ matrix.plan-file }}"
          exit 1
        fi
        echo "Test plan file exists: ${{ matrix.plan-file }}"

    - name: Test Action - ${{ matrix.scenario }}
      id: test-strata
      uses: ./
      with:
        plan-file: ${{ matrix.plan-file }}
        output-format: ${{ matrix.output-format }}
        show-details: ${{ matrix.show-details }}
        comment-on-pr: ${{ matrix.comment-on-pr }}
        config-file: ${{ matrix.config-file || '' }}
        danger-threshold: 3

    - name: Verify Action Outputs
      run: |
        echo "Testing action outputs for scenario: ${{ matrix.scenario }}"
        
        # Check that required outputs are set
        if [ -z "${{ steps.test-strata.outputs.summary }}" ]; then
          echo "ERROR: summary output is empty"
          exit 1
        fi
        
        if [ -z "${{ steps.test-strata.outputs.has-changes }}" ]; then
          echo "ERROR: has-changes output is empty"
          exit 1
        fi
        
        if [ -z "${{ steps.test-strata.outputs.has-dangers }}" ]; then
          echo "ERROR: has-dangers output is empty"
          exit 1
        fi
        
        if [ -z "${{ steps.test-strata.outputs.change-count }}" ]; then
          echo "ERROR: change-count output is empty"
          exit 1
        fi
        
        if [ -z "${{ steps.test-strata.outputs.danger-count }}" ]; then
          echo "ERROR: danger-count output is empty"
          exit 1
        fi
        
        # Validate output types
        if [[ ! "${{ steps.test-strata.outputs.has-changes }}" =~ ^(true|false)$ ]]; then
          echo "ERROR: has-changes should be boolean, got: ${{ steps.test-strata.outputs.has-changes }}"
          exit 1
        fi
        
        if [[ ! "${{ steps.test-strata.outputs.has-dangers }}" =~ ^(true|false)$ ]]; then
          echo "ERROR: has-dangers should be boolean, got: ${{ steps.test-strata.outputs.has-dangers }}"
          exit 1
        fi
        
        if [[ ! "${{ steps.test-strata.outputs.change-count }}" =~ ^[0-9]+$ ]]; then
          echo "ERROR: change-count should be numeric, got: ${{ steps.test-strata.outputs.change-count }}"
          exit 1
        fi
        
        if [[ ! "${{ steps.test-strata.outputs.danger-count }}" =~ ^[0-9]+$ ]]; then
          echo "ERROR: danger-count should be numeric, got: ${{ steps.test-strata.outputs.danger-count }}"
          exit 1
        fi
        
        echo "âœ… All outputs are valid"
        echo "Summary length: $(echo '${{ steps.test-strata.outputs.summary }}' | wc -c)"
        echo "Has changes: ${{ steps.test-strata.outputs.has-changes }}"
        echo "Has dangers: ${{ steps.test-strata.outputs.has-dangers }}"
        echo "Change count: ${{ steps.test-strata.outputs.change-count }}"
        echo "Danger count: ${{ steps.test-strata.outputs.danger-count }}"

    - name: Test JSON Output (for json-output scenario)
      if: matrix.scenario == 'json-output'
      run: |
        echo "Testing JSON output parsing"
        json_summary='${{ steps.test-strata.outputs.json-summary }}'
        
        if [ -z "$json_summary" ]; then
          echo "ERROR: json-summary output is empty"
          exit 1
        fi
        
        # Validate JSON format
        if command -v jq >/dev/null 2>&1; then
          if ! echo "$json_summary" | jq . >/dev/null 2>&1; then
            echo "ERROR: json-summary is not valid JSON"
            echo "Content: $json_summary"
            exit 1
          fi
          echo "âœ… JSON output is valid"
        else
          echo "âš ï¸ jq not available, skipping JSON validation"
        fi

  # Test error scenarios
  test-error-scenarios:
    name: Test Error Scenarios
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Test with non-existent plan file
      id: test-missing-file
      uses: ./
      continue-on-error: true
      with:
        plan-file: non-existent-file.tfplan
        comment-on-pr: false

    - name: Verify error handling for missing file
      run: |
        if [ "${{ steps.test-missing-file.outcome }}" = "success" ]; then
          echo "ERROR: Action should have failed with missing file"
          exit 1
        fi
        echo "âœ… Action correctly failed with missing file"

    - name: Test with invalid output format
      id: test-invalid-format
      uses: ./
      with:
        plan-file: samples/websample.json
        output-format: invalid-format
        comment-on-pr: false

    - name: Verify invalid format handling
      run: |
        # Action should succeed but use default format
        if [ "${{ steps.test-invalid-format.outcome }}" != "success" ]; then
          echo "ERROR: Action should succeed with invalid format (using default)"
          exit 1
        fi
        echo "âœ… Action handled invalid format gracefully"

  # Test PR comment functionality (only on PRs)
  test-pr-comments:
    name: Test PR Comments
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Test PR Comment Creation
      id: test-pr-comment
      uses: ./
      with:
        plan-file: samples/websample.json
        comment-on-pr: true
        update-comment: false
        comment-header: "ðŸ§ª Test Infrastructure Changes"

    - name: Test PR Comment Update
      id: test-pr-update
      uses: ./
      with:
        plan-file: samples/k8ssample.json
        comment-on-pr: true
        update-comment: true
        comment-header: "ðŸ§ª Test Infrastructure Changes"

    - name: Verify PR comment functionality
      run: |
        echo "âœ… PR comment functionality tested"
        echo "Comment creation outcome: ${{ steps.test-pr-comment.outcome }}"
        echo "Comment update outcome: ${{ steps.test-pr-update.outcome }}"

  # Test caching functionality
  test-caching:
    name: Test Binary Caching
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: First run (should download binary)
      id: first-run
      uses: ./
      with:
        plan-file: samples/websample.json
        comment-on-pr: false

    - name: Second run (should use cached binary)
      id: second-run
      uses: ./
      with:
        plan-file: samples/k8ssample.json
        comment-on-pr: false

    - name: Verify caching worked
      run: |
        echo "âœ… Caching functionality tested"
        echo "First run outcome: ${{ steps.first-run.outcome }}"
        echo "Second run outcome: ${{ steps.second-run.outcome }}"

  # Test with different Terraform plan formats
  test-plan-formats:
    name: Test Plan Formats
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Test with web sample (JSON format)
      uses: ./
      with:
        plan-file: samples/websample.json
        comment-on-pr: false

    - name: Test with k8s sample (JSON format)
      uses: ./
      with:
        plan-file: samples/k8ssample.json
        comment-on-pr: false

    - name: Test with danger sample (JSON format)
      uses: ./
      with:
        plan-file: samples/danger-sample.json
        comment-on-pr: false

  # Performance test
  test-performance:
    name: Test Performance
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Performance test with timing
      run: |
        echo "Starting performance test..."
        start_time=$(date +%s)
        
        # Run the action
        time ./action.sh
        
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        
        echo "Action completed in ${duration} seconds"
        
        # Fail if it takes too long (more than 5 minutes)
        if [ $duration -gt 300 ]; then
          echo "ERROR: Action took too long to complete (${duration}s > 300s)"
          exit 1
        fi
        
        echo "âœ… Performance test passed"
      env:
        INPUT_PLAN_FILE: samples/websample.json
        INPUT_OUTPUT_FORMAT: markdown
        INPUT_SHOW_DETAILS: false
        INPUT_COMMENT_ON_PR: false

  # Test summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-action, test-error-scenarios, test-caching, test-plan-formats, test-performance]
    if: always()
    steps:
    - name: Test Results Summary
      run: |
        echo "## ðŸ§ª GitHub Action Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Action Tests | ${{ needs.test-action.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Error Scenarios | ${{ needs.test-error-scenarios.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Caching | ${{ needs.test-caching.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Plan Formats | ${{ needs.test-plan-formats.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance | ${{ needs.test-performance.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Overall result
        if [[ "${{ needs.test-action.result }}" == "success" && 
              "${{ needs.test-error-scenarios.result }}" == "success" && 
              "${{ needs.test-caching.result }}" == "success" && 
              "${{ needs.test-plan-formats.result }}" == "success" && 
              "${{ needs.test-performance.result }}" == "success" ]]; then
          echo "### âœ… All tests passed!" >> $GITHUB_STEP_SUMMARY
          echo "The GitHub Action is working correctly across all test scenarios."
        else
          echo "### âŒ Some tests failed!" >> $GITHUB_STEP_SUMMARY
          echo "Please check the individual test results above."
          exit 1
        fi